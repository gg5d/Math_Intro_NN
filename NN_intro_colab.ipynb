{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gg5d/Math_Intro_NN/blob/master/NN_intro_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95e5afd7-df62-4b8c-a47a-7228221364f4",
      "metadata": {
        "id": "95e5afd7-df62-4b8c-a47a-7228221364f4"
      },
      "source": [
        "Math Intro to NN for DS1001"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c30de97d-c522-423a-a259-579bc1b0c399",
      "metadata": {
        "id": "c30de97d-c522-423a-a259-579bc1b0c399"
      },
      "source": [
        "Gianluca Guadagni - gg5d - April 2024"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed6215da",
      "metadata": {
        "id": "ed6215da"
      },
      "source": [
        "![full NN](https://github.com/gg5d/Math_Intro_NN/blob/master/screenshots_NN/full_NN.png?raw=true\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7376f21",
      "metadata": {
        "id": "b7376f21"
      },
      "source": [
        "Let's be a little bit formal"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0eb792f",
      "metadata": {
        "id": "c0eb792f"
      },
      "source": [
        "<img src=\"https://github.com/gg5d/Math_Intro_NN/blob/master/screenshots_NN/one_step_matrix_NN.png?raw=true\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "967af4f9",
      "metadata": {
        "id": "967af4f9"
      },
      "source": [
        "We are going to play with a (small but) real NN composed by 1) an input layer with one node, 2) one hidden layer with two nodes, and 3) an output layer with one node"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86d051c9",
      "metadata": {
        "id": "86d051c9"
      },
      "source": [
        "![small NN](https://github.com/gg5d/Math_Intro_NN/blob/master/screenshots_NN/small_NN.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2862a48e",
      "metadata": {
        "id": "2862a48e"
      },
      "source": [
        "In Machine Learning, the process of going from the green input to the red output is called FORWARD PROPAGATION, the same process, in Mathematics, is called a FUNCTION ðŸ˜Š."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d02cc5b",
      "metadata": {
        "id": "4d02cc5b"
      },
      "source": [
        "Based on the matrix formulas outlined above, we are going to write the forward propagation in details for the NN(1,2,1): one node as input layer, 2 nodes as hidden layer, and one node as output layer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af797165",
      "metadata": {
        "id": "af797165"
      },
      "source": [
        "There are 7 parameters in this model, 4 W's and 3 b's."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21bdb780",
      "metadata": {
        "id": "21bdb780"
      },
      "source": [
        "Let's start with the first half"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bcb5158",
      "metadata": {
        "id": "7bcb5158"
      },
      "source": [
        "![LHS NN](https://github.com/gg5d/Math_Intro_NN/blob/master/screenshots_NN/small_NN_LHS.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbf4dc91",
      "metadata": {
        "id": "cbf4dc91"
      },
      "source": [
        "Now the second half"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fe7d8a8",
      "metadata": {
        "id": "2fe7d8a8"
      },
      "source": [
        "![RHS NN](https://github.com/gg5d/Math_Intro_NN/blob/master/screenshots_NN/small_NN_RHS.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae1eb20b",
      "metadata": {
        "id": "ae1eb20b"
      },
      "source": [
        "Now, we are going to put the two halves together"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b7c0250",
      "metadata": {
        "id": "4b7c0250"
      },
      "source": [
        "![full NN formula](https://github.com/gg5d/Math_Intro_NN/blob/master/screenshots_NN/small_NN_full_output.png?raw=true)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28d5b14a",
      "metadata": {
        "id": "28d5b14a"
      },
      "source": [
        "Enough Math, let's do some coding and implement the NN. We will generate the forward propagation function. Look at the second row above as the best reference for the code."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e532f1d",
      "metadata": {
        "id": "6e532f1d"
      },
      "source": [
        "Some imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7012a211",
      "metadata": {
        "id": "7012a211"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4d55a50",
      "metadata": {
        "id": "e4d55a50"
      },
      "outputs": [],
      "source": [
        "# Define the number of input, hidden, and output nodes\n",
        "input_nodes = 1\n",
        "hidden_nodes = 2\n",
        "output_nodes = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9cc156d",
      "metadata": {
        "id": "d9cc156d"
      },
      "source": [
        "Define the **activation** function **$\\mathbf{\\sigma(x)}$**, and its derivative **$\\mathbf{\\sigma'(x)}$**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4973ab9",
      "metadata": {
        "id": "c4973ab9"
      },
      "outputs": [],
      "source": [
        "def sigma(x):\n",
        "    return x*(x>0)               # ReLU\n",
        "#    return 1 / (1 + np.exp(-x)) # sigmoid\n",
        "\n",
        "# Define the derivative of the activation function\n",
        "def sigma_derivative(x):\n",
        "    return 1*(x>0)                # ReLU\n",
        "#    return sigma(x) * (1 - sigma(x)) #sigmoid\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a range of values from -10 to 10 to plot\n",
        "x_values = np.linspace(-10, 10, 200)\n",
        "sigma_values = sigma(x_values)\n",
        "sigma_derivative_values = sigma_derivative(x_values)\n",
        "\n",
        "# Plotting both functions\n",
        "plt.plot(x_values, sigma_values, label='Activation Function', color='blue')\n",
        "plt.plot(x_values, sigma_derivative_values, label='Derivative of Activation Function', color='orange', linestyle='--')\n",
        "plt.title('Activation Function and its Derivative')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "twOWxr48H3lm"
      },
      "id": "twOWxr48H3lm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "d76ea9ec",
      "metadata": {
        "id": "d76ea9ec"
      },
      "source": [
        "Define the **forward propagation** function [This is the function that will compute $\\mathbf{a}^{(1)}$ and $\\mathbf{a}^{(2)}$]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fdea751",
      "metadata": {
        "id": "6fdea751"
      },
      "outputs": [],
      "source": [
        "# Define the forward propagation function\n",
        "def forward_propagation(x):\n",
        "    z1 = np.dot(W1, x) + b1\n",
        "    a1 = sigma(z1)\n",
        "    z2 = np.dot(W2, a1) + b2\n",
        "    a2 = sigma(z2)\n",
        "    return a1, a2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b347e5ae",
      "metadata": {
        "id": "b347e5ae"
      },
      "source": [
        "For convenience, we also define a range of values for $x$, and a function to evaluate the output $y$ for each of the $x$'s. We'll need this for the plots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d08b41dd",
      "metadata": {
        "id": "d08b41dd"
      },
      "outputs": [],
      "source": [
        "# Generate 100 values for x\n",
        "X = np.linspace(start=-1, stop=2,num=100)\n",
        "\n",
        "# Evaluate the output y based on the forward propagation\n",
        "def output(X):\n",
        "    outp = np.array([])\n",
        "    for j in range(X.shape[0]):\n",
        "        x = X[j].reshape(input_nodes, 1)\n",
        "        a1, a2 = forward_propagation(x)\n",
        "        outp = np.append(outp,a2)\n",
        "    return outp"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc19e705",
      "metadata": {
        "id": "bc19e705"
      },
      "source": [
        "Now, let's try the NN for some choice of the parameters W's and b'.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1144d72",
      "metadata": {
        "id": "e1144d72"
      },
      "source": [
        "Initialize weights $W$ and biases $b$ (close to the actual values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5462425d",
      "metadata": {
        "id": "5462425d"
      },
      "outputs": [],
      "source": [
        "#\n",
        "W1 = np.array([[1.5],[1.5]]) # np.random.randn(hidden_nodes, input_nodes)\n",
        "b1 = np.array([[0.8],[-.5]]) # np.zeros((hidden_nodes, 1))\n",
        "W2 = np.array([[1.0,-2.4]]) #np.random.randn(output_nodes, hidden_nodes)\n",
        "b2 = np.array([[1.0]]) #np.zeros((output_nodes, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c3c6966",
      "metadata": {
        "id": "9c3c6966"
      },
      "source": [
        "Let's plot the output of the NN, $y$ vs $x$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca22d3b9",
      "metadata": {
        "id": "ca22d3b9"
      },
      "outputs": [],
      "source": [
        "X = np.linspace(start=-1, stop=2,num=100)\n",
        "Y = output(X)\n",
        "plt.plot(X,Y, label=\"NN\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52c0d510",
      "metadata": {
        "id": "52c0d510"
      },
      "source": [
        "With the following choice of the parameters, we get a triangular function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "048f2ca4",
      "metadata": {
        "id": "048f2ca4"
      },
      "outputs": [],
      "source": [
        "#\n",
        "W1 = np.array([[1.0],[1.0]]) # np.random.randn(hidden_nodes, input_nodes)\n",
        "b1 = np.array([[0.0],[-.5]]) # np.zeros((hidden_nodes, 1))\n",
        "W2 = np.array([[2.0,-4.0]]) #np.random.randn(output_nodes, hidden_nodes)\n",
        "b2 = np.array([[0.0]]) #np.zeros((output_nodes, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a74205d5",
      "metadata": {
        "id": "a74205d5"
      },
      "outputs": [],
      "source": [
        "X = np.linspace(start=-1, stop=2,num=100)\n",
        "Y = output(X)\n",
        "plt.plot(X,Y, label=\"NN\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9a73c53",
      "metadata": {
        "id": "b9a73c53"
      },
      "source": [
        "The triangular function can also be defined, as a mathematical function, by\n",
        "\n",
        "$$\n",
        "f(x) = \\begin{cases}\n",
        "0 & x \\leq 0 \\\\\n",
        "2x & 0<x<0.5 \\\\\n",
        "2-2x & 0.5 \\leq x <1 \\\\\n",
        "0 & 1 \\leq x  \\end{cases}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b0ddccc",
      "metadata": {
        "id": "9b0ddccc"
      },
      "outputs": [],
      "source": [
        "def tri(x):\n",
        "    return 2*x*(x>0)*(x<.5)+(2-2*x)*(x>=.5)*(x<1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50413221",
      "metadata": {
        "id": "50413221"
      },
      "source": [
        "and plotted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d303c02",
      "metadata": {
        "id": "4d303c02"
      },
      "outputs": [],
      "source": [
        "y =tri(X)\n",
        "\n",
        "plt.plot(X,y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a47e3e42",
      "metadata": {
        "id": "a47e3e42"
      },
      "source": [
        "As a matter of fact, there is a theorem that says: a NN with one hidden layer with n-nodes and ReLU activations can generate all piecewise linear continuous functions with at most n+1 breakpoints.  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6456f94b",
      "metadata": {
        "id": "6456f94b"
      },
      "source": [
        "BIG QUESTION: If we do not know the correct values of $W$ and $b$, how do we determine the parameters W's and b' that would give us the triangular function?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05d3c9c1",
      "metadata": {
        "id": "05d3c9c1"
      },
      "source": [
        "We would be happy if the output $y$ were close in value to the value of the function at that $x$, $f(x)$. Let's measure $y$ \"$\\textbf{closeness}$\" to the function $f(x)$ by a **Cost** function"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1feb7a87",
      "metadata": {
        "id": "1feb7a87"
      },
      "source": [
        "COST FUNCTION"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "121fa50f",
      "metadata": {
        "id": "121fa50f"
      },
      "source": [
        "![Cost function](https://github.com/gg5d/Math_Intro_NN/blob/master/screenshots_NN/small_NN_cost_function.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fd80624",
      "metadata": {
        "id": "3fd80624"
      },
      "source": [
        "IMPORTANT FACT:  The output $y$ of the NN is a function of x, but also of  four W's and three b's."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63dbfd7d",
      "metadata": {
        "id": "63dbfd7d"
      },
      "source": [
        "GOAL:  find values of all W's and b's that would *minimize* the Cost function"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45341424",
      "metadata": {
        "id": "45341424"
      },
      "source": [
        "First STEP: since we have no clue, we start with random or otherwise chosen values for W's and b's"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d38914c",
      "metadata": {
        "id": "0d38914c"
      },
      "source": [
        "ALGORITHM: After the first step, the cost function is most likely not zero [if it is zero you are done! Congratulations!], and we need to tweak/change the parameters $W$'s and $b$'s. We use an idea by Newton to modify the parameters W's and b's. And we will keep repeating it until we are happy with the result.\n",
        "\n",
        "I am going to show you on the board how the algorithm works in one dimension, for higher dimensions it is called **Gradiant Descent***\n",
        "https://en.wikipedia.org/wiki/Gradient_descent"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ad0c32a",
      "metadata": {
        "id": "1ad0c32a"
      },
      "source": [
        "For the gradient/partial derivatives we need a lot of derivatives [CHAIN RULE], here they are:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "806c911b",
      "metadata": {
        "id": "806c911b"
      },
      "source": [
        "![partial derivatives](https://github.com/gg5d/Math_Intro_NN/blob/master/screenshots_NN/small_NN_gradients.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81b3680f",
      "metadata": {
        "id": "81b3680f"
      },
      "source": [
        "In one iteration of the algorithm, each parameter will be modified as follows:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36f6c745",
      "metadata": {
        "id": "36f6c745"
      },
      "source": [
        "![Gradient Descent](https://github.com/gg5d/Math_Intro_NN/blob/master/screenshots_NN/small_NN_gradient_descent.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ebf9063",
      "metadata": {
        "id": "8ebf9063"
      },
      "source": [
        "$h$ is called *learning rate*."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d0c9c9e",
      "metadata": {
        "id": "7d0c9c9e"
      },
      "source": [
        "This algorithm is called **backpropagation**. This is an implementation in python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa482f45",
      "metadata": {
        "id": "aa482f45"
      },
      "outputs": [],
      "source": [
        "# Define the cost function\n",
        "def cost_function(y, y_hat):\n",
        "    return 0.5 * np.sum((y - y_hat)**2)\n",
        "\n",
        "# Define the gradient function\n",
        "def gradient(x, y, a1, a2):\n",
        "    delta2 = (a2 - y) * sigma_derivative(a2)\n",
        "    dW2 = np.dot(delta2, a1.T)\n",
        "    db2 = delta2\n",
        "    delta1 = np.dot(W2.T, delta2) * sigma_derivative(a1)\n",
        "    dW1 = np.dot(delta1, x.T)\n",
        "    db1 = delta1\n",
        "    return dW1, db1, dW2, db2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81e23841",
      "metadata": {
        "id": "81e23841"
      },
      "source": [
        "Let's run the backpropagation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f49851d5",
      "metadata": {
        "id": "f49851d5"
      },
      "outputs": [],
      "source": [
        "# Initialize weights and biases close to the actual values\n",
        "W1 = np.array([[1.8],[1.8]]) # np.random.randn(hidden_nodes, input_nodes)\n",
        "b1 = np.array([[0.8],[-.5]]) # np.zeros((hidden_nodes, 1))\n",
        "W2 = np.array([[1.0,-2.4]]) #np.random.randn(output_nodes, hidden_nodes)\n",
        "b2 = np.array([[1.0]]) #np.zeros((output_nodes, 1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28357c05",
      "metadata": {
        "id": "28357c05"
      },
      "outputs": [],
      "source": [
        "# You can rerun this cell as many times as you need,\n",
        "# until your function is close enough to the triangular function\n",
        "\n",
        "\n",
        "# Define the learning rate and number of iterations (the number of times we will repeat backpropagation)\n",
        "learning_rate = 0.1\n",
        "num_iterations = 100\n",
        "\n",
        "# Perform gradient descent\n",
        "for i in range(num_iterations):\n",
        "    # Initialize the gradients\n",
        "    dW1 = np.zeros_like(W1)\n",
        "    db1 = np.zeros_like(b1)\n",
        "    dW2 = np.zeros_like(W2)\n",
        "    db2 = np.zeros_like(b2)\n",
        "\n",
        "    # Compute the gradients for each sample\n",
        "    for j in range(X.shape[0]):\n",
        "        x = X[j].reshape(input_nodes, 1)\n",
        "        y_true = y[j].reshape(output_nodes, 1)\n",
        "\n",
        "        # Forward propagation\n",
        "        a1, a2 = forward_propagation(x)\n",
        "\n",
        "        # Compute the cost\n",
        "        cost = cost_function(y_true, a2)\n",
        "\n",
        "        # Gradient\n",
        "        dW1_j, db1_j, dW2_j, db2_j = gradient(x, y_true, a1, a2)\n",
        "\n",
        "        # Accumulate the gradients\n",
        "        dW1 += dW1_j\n",
        "        db1 += db1_j\n",
        "        dW2 += dW2_j\n",
        "        db2 += db2_j\n",
        "\n",
        "    # Average the gradients over all samples\n",
        "    dW1 /= X.shape[0]\n",
        "    db1 /= X.shape[0]\n",
        "    dW2 /= X.shape[0]\n",
        "    db2 /= X.shape[0]\n",
        "\n",
        "    # Update the weights and biases: backpropagation\n",
        "    W1 -= learning_rate*dW1\n",
        "    b1 -= learning_rate*db1\n",
        "    W2 -= learning_rate*dW2\n",
        "    b2 -= learning_rate*db2\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15ea8585",
      "metadata": {
        "id": "15ea8585"
      },
      "source": [
        "Let's see what we got:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "613ddb08",
      "metadata": {
        "id": "613ddb08"
      },
      "outputs": [],
      "source": [
        "print(W1,b1,W2,b2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "101a9ffb",
      "metadata": {
        "id": "101a9ffb"
      },
      "outputs": [],
      "source": [
        "Y = output(X)\n",
        "plt.plot(X,Y, label=\"NN\")\n",
        "plt.plot(X,y, color='r', label=\"Tri\")\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0435a598",
      "metadata": {
        "id": "0435a598"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}